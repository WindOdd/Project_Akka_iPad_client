import SwiftUI
import Speech
import AVFoundation
import Combine

struct ContentView: View {
    @StateObject private var voiceManager = VoiceManager()
    
    var body: some View {
        VStack(spacing: 30) {
            Text("èªéŸ³è¾¨è­˜èˆ‡æœ—è®€")
                .font(.largeTitle)
                .fontWeight(.bold)
            
            // éŒ„éŸ³æŒ‰éˆ•
            Button(action: {
                voiceManager.toggleRecording()
            }) {
                VStack {
                    Image(systemName: voiceManager.isRecording ? "stop.circle.fill" : "mic.circle.fill")
                        .resizable()
                        .frame(width: 80, height: 80)
                        .foregroundColor(voiceManager.isRecording ? .red : .blue)
                    
                    Text(voiceManager.isRecording ? "åœæ­¢éŒ„éŸ³" : "é–‹å§‹éŒ„éŸ³")
                        .font(.headline)
                        .padding(.top, 8)
                }
            }
            .padding()
            
            // ç‹€æ…‹é¡¯ç¤º
            Text(voiceManager.statusMessage)
                .font(.subheadline)
                .foregroundColor(.gray)
                .padding(.horizontal)
            
            // è¾¨è­˜çµæœé¡¯ç¤º
            ScrollView {
                Text(voiceManager.recognizedText.isEmpty ? "è¾¨è­˜çµæœæœƒé¡¯ç¤ºåœ¨é€™è£¡..." : voiceManager.recognizedText)
                    .font(.body)
                    .padding()
                    .frame(maxWidth: .infinity, alignment: .leading)
                    .background(Color.gray.opacity(0.1))
                    .cornerRadius(10)
            }
            .frame(maxHeight: 300)
            .padding(.horizontal)
            
            // æœ—è®€æŒ‰éˆ•
            Button(action: {
                voiceManager.speakText()
            }) {
                HStack {
                    Image(systemName: "speaker.wave.2.fill")
                    Text("æœ—è®€æ–‡å­—")
                }
                .font(.headline)
                .foregroundColor(.white)
                .padding()
                .background(voiceManager.recognizedText.isEmpty ? Color.gray : Color.green)
                .cornerRadius(10)
            }
            .disabled(voiceManager.recognizedText.isEmpty)
            .padding(.horizontal)
            
            Spacer()
        }
        .padding()
        .onAppear {
            voiceManager.requestPermissions()
        }
    }
}

class VoiceManager: NSObject, ObservableObject {
    @Published var recognizedText = ""
    @Published var isRecording = false
    @Published var statusMessage = "æº–å‚™å°±ç·’"
    
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-TW"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private var audioEngine = AVAudioEngine()
    private let speechSynthesizer = AVSpeechSynthesizer()
    
    override init() {
        super.init()
        speechSynthesizer.delegate = self
    }
    
    // è«‹æ±‚æ¬Šé™
    func requestPermissions() {
        SFSpeechRecognizer.requestAuthorization { status in
            DispatchQueue.main.async {
                switch status {
                case .authorized:
                    self.statusMessage = "å·²æˆæ¬ŠèªéŸ³è¾¨è­˜"
                case .denied:
                    self.statusMessage = "èªéŸ³è¾¨è­˜æ¬Šé™è¢«æ‹’çµ•"
                case .restricted:
                    self.statusMessage = "èªéŸ³è¾¨è­˜å—é™"
                case .notDetermined:
                    self.statusMessage = "å°šæœªæ±ºå®šèªéŸ³è¾¨è­˜æ¬Šé™"
                @unknown default:
                    self.statusMessage = "æœªçŸ¥çš„æˆæ¬Šç‹€æ…‹"
                }
            }
        }
        
        AVAudioSession.sharedInstance().requestRecordPermission { allowed in
            DispatchQueue.main.async {
                if !allowed {
                    self.statusMessage = "éº¥å…‹é¢¨æ¬Šé™è¢«æ‹’çµ•"
                }
            }
        }
    }
    
    // åˆ‡æ›éŒ„éŸ³ç‹€æ…‹
    func toggleRecording() {
        if isRecording {
            stopRecording()
        } else {
            startRecording()
        }
    }
    
    // é–‹å§‹éŒ„éŸ³
    func startRecording() {
        // é‡ç½®ä¹‹å‰çš„ä»»å‹™
        if recognitionTask != nil {
            recognitionTask?.cancel()
            recognitionTask = nil
        }
        
        // é…ç½®éŸ³è¨Šæœƒè©±
        let audioSession = AVAudioSession.sharedInstance()
        do {
            try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        } catch {
            statusMessage = "éŸ³è¨Šæœƒè©±è¨­å®šå¤±æ•—"
            return
        }
        
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        
        let inputNode = audioEngine.inputNode
        guard let recognitionRequest = recognitionRequest else {
            statusMessage = "ç„¡æ³•å»ºç«‹è¾¨è­˜è«‹æ±‚"
            return
        }
        
        recognitionRequest.shouldReportPartialResults = true
        
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            guard let self = self else { return }
            
            var isFinal = false
            
            if let result = result {
                DispatchQueue.main.async {
                    self.recognizedText = result.bestTranscription.formattedString
                    print("è¾¨è­˜ä¸­ï¼š\(self.recognizedText)")
                }
                isFinal = result.isFinal
            }
            
            if error != nil || isFinal {
                self.audioEngine.stop()
                    inputNode.removeTap(onBus: 0)
                    
                    // å®Œå…¨éŠ·æ¯€ä¸¦é‡æ–°å»ºç«‹éŸ³è¨Šå¼•æ“
                    self.audioEngine.reset()
                    self.audioEngine = AVAudioEngine()
                    print("éŸ³è¨Šå¼•æ“å·²å®Œå…¨é‡å»º")
                    
                    self.recognitionRequest = nil
                    self.recognitionTask = nil
                
                // ä¿å­˜æœ€çµ‚è¾¨è­˜çµæœ
                let finalText = self.recognizedText
                
                DispatchQueue.main.async {
                    self.isRecording = false
                    self.statusMessage = "éŒ„éŸ³çµæŸ"
                    print("æœ€çµ‚è¾¨è­˜çµæœï¼š\(finalText)")
                    
                    // åœç”¨éŒ„éŸ³éŸ³è¨Šæœƒè©±
                    let audioSession = AVAudioSession.sharedInstance()
                    do {
                        try audioSession.setActive(false, options: .notifyOthersOnDeactivation)
                    } catch {
                        print("åœç”¨éŸ³è¨Šæœƒè©±å¤±æ•—ï¼š\(error.localizedDescription)")
                    }
                    
                    // å»¶é² 1 ç§’å¾Œè‡ªå‹•æ’­æ”¾
                    DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
                        if !finalText.isEmpty {
                            self.speakText()
                        } else {
                            self.statusMessage = "æ²’æœ‰è¾¨è­˜åˆ°æ–‡å­—"
                        }
                    }
                }
            }
        }
        
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
            self.recognitionRequest?.append(buffer)
        }
        
        audioEngine.prepare()
        
        do {
            try audioEngine.start()
            isRecording = true
            statusMessage = "æ­£åœ¨éŒ„éŸ³..."
        } catch {
            statusMessage = "éŸ³è¨Šå¼•æ“å•Ÿå‹•å¤±æ•—"
        }
    }
    
    // åœæ­¢éŒ„éŸ³
    func stopRecording() {
        recognitionRequest?.endAudio()
        statusMessage = "è™•ç†ä¸­..."
    }
    
    // æœ—è®€æ–‡å­—
    // æœ—è®€æ–‡å­—
    func speakText() {
        print("========== é–‹å§‹æœ—è®€æµç¨‹ ==========")
        print("1. è¦æœ—è®€çš„æ–‡å­—ï¼š\(recognizedText)")
        print("2. æ–‡å­—é•·åº¦ï¼š\(recognizedText.count)")
        
        // æª¢æŸ¥ audioEngine ç‹€æ…‹
        print("3. audioEngine.isRunning: \(audioEngine.isRunning)")
        
        // æª¢æŸ¥ speechSynthesizer ç‹€æ…‹
        print("4. speechSynthesizer.isSpeaking: \(speechSynthesizer.isSpeaking)")
        print("5. speechSynthesizer.isPaused: \(speechSynthesizer.isPaused)")
        
        // ç²å–ç•¶å‰éŸ³è¨Šæœƒè©±ç‹€æ…‹
        let audioSession = AVAudioSession.sharedInstance()
        print("6. ç•¶å‰éŸ³è¨Šæœƒè©±é¡åˆ¥: \(audioSession.category.rawValue)")
        print("7. ç•¶å‰éŸ³è¨Šæœƒè©±æ¨¡å¼: \(audioSession.mode.rawValue)")
        print("8. éŸ³è¨Šæœƒè©±æ˜¯å¦å•Ÿç”¨: \(audioSession.isOtherAudioPlaying)")
        
        // é‡æ–°è¨­å®šéŸ³è¨Šæœƒè©±ç‚ºæ’­æ”¾æ¨¡å¼
        do {
            print("9. æº–å‚™è¨­å®šéŸ³è¨Šæœƒè©±ç‚ºæ’­æ”¾æ¨¡å¼...")
            try audioSession.setCategory(.playback, mode: .default, options: [])
            print("10. éŸ³è¨Šæœƒè©±é¡åˆ¥è¨­å®šæˆåŠŸ")
            
            try audioSession.setActive(true, options: [])
            print("11. éŸ³è¨Šæœƒè©±å•Ÿç”¨æˆåŠŸ")
            
            // å†æ¬¡æª¢æŸ¥ç‹€æ…‹
            print("12. è¨­å®šå¾ŒéŸ³è¨Šæœƒè©±é¡åˆ¥: \(audioSession.category.rawValue)")
            print("13. è¨­å®šå¾ŒéŸ³è¨Šæœƒè©±æ˜¯å¦å•Ÿç”¨: \(audioSession.isOtherAudioPlaying)")
            
        } catch let error as NSError {
            print("âŒ éŸ³è¨Šæœƒè©±è¨­å®šå¤±æ•—")
            print("éŒ¯èª¤ä»£ç¢¼: \(error.code)")
            print("éŒ¯èª¤æè¿°: \(error.localizedDescription)")
            print("éŒ¯èª¤è³‡è¨Š: \(error.userInfo)")
            statusMessage = "éŸ³è¨Šè¨­å®šå¤±æ•—: \(error.localizedDescription)"
            return
        }
        
        if speechSynthesizer.isSpeaking {
            print("14. åœæ­¢ç¾æœ‰çš„æœ—è®€...")
            speechSynthesizer.stopSpeaking(at: .immediate)
        }
        
        // å»ºç«‹èªéŸ³å…§å®¹
        let utterance = AVSpeechUtterance(string: recognizedText)
        utterance.voice = AVSpeechSynthesisVoice(language: "zh-TW")
        utterance.rate = 0.5
        utterance.pitchMultiplier = 1.0
        utterance.volume = 1.0
        
        print("15. AVSpeechUtterance å»ºç«‹æˆåŠŸ")
        print("16. èªéŸ³èªè¨€: \(utterance.voice?.language ?? "nil")")
        print("17. èªéŸ³é€Ÿåº¦: \(utterance.rate)")
        print("18. èªéŸ³éŸ³é‡: \(utterance.volume)")
        
        // æª¢æŸ¥å¯ç”¨çš„èªéŸ³
        let availableVoices = AVSpeechSynthesisVoice.speechVoices()
        let zhVoices = availableVoices.filter { $0.language.hasPrefix("zh") }
        print("19. ç³»çµ±å¯ç”¨çš„ä¸­æ–‡èªéŸ³æ•¸é‡: \(zhVoices.count)")
        for (index, voice) in zhVoices.enumerated() {
            print("    èªéŸ³ \(index): \(voice.language) - \(voice.name)")
        }
        
        print("20. æº–å‚™å‘¼å« speechSynthesizer.speak()...")
        statusMessage = "æ­£åœ¨æœ—è®€..."
        
        speechSynthesizer.speak(utterance)
        
        print("21. speechSynthesizer.speak() å·²å‘¼å«")
        print("22. å‘¼å«å¾Œ isSpeaking: \(speechSynthesizer.isSpeaking)")
        print("========== æœ—è®€æµç¨‹çµæŸ ==========\n")
    }
}

extension VoiceManager: AVSpeechSynthesizerDelegate {
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didStart utterance: AVSpeechUtterance) {
        print("ğŸ¤ æœ—è®€å·²é–‹å§‹")
        DispatchQueue.main.async {
            self.statusMessage = "æ­£åœ¨æœ—è®€..."
        }
    }
    
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didFinish utterance: AVSpeechUtterance) {
        print("âœ… æœ—è®€å·²å®Œæˆ")
        DispatchQueue.main.async {
            self.statusMessage = "æœ—è®€å®Œæˆ"
        }
    }
    
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didPause utterance: AVSpeechUtterance) {
        print("â¸ï¸ æœ—è®€å·²æš«åœ")
    }
    
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didContinue utterance: AVSpeechUtterance) {
        print("â–¶ï¸ æœ—è®€å·²ç¹¼çºŒ")
    }
    
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didCancel utterance: AVSpeechUtterance) {
        print("âŒ æœ—è®€å·²å–æ¶ˆ")
    }
    
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, willSpeakRangeOfSpeechString characterRange: NSRange, utterance: AVSpeechUtterance) {
        print("ğŸ“ æ­£åœ¨æœ—è®€å­—å…ƒç¯„åœ: \(characterRange)")
    }
}

#Preview {
    ContentView()
}
