import Foundation
import AVFoundation
import Speech  // ğŸ‘ˆ å¼•å…¥ Apple èªéŸ³æ¡†æ¶
import WhisperKit
import Combine

// MARK: - æ¨¡å‹å®šç¾© (é›™å¼•æ“æ•´åˆ)
enum WhisperModel: String, CaseIterable, Identifiable {
    // ğŸ Apple å…§å»º
    case native = "native_apple"
    
    // ğŸ¤– OpenAI Whisper ç³»åˆ—
    case base = "base"                                      // User æŒ‡å®šæ¸¬è©¦
    case openaiLargeV3Turbo_626MB = "openai_whisper-large-v3-v20240930_626MB" // åŸæœ¬çš„ Turbo
    
    var id: String { self.rawValue }
    
    var displayName: String {
        switch self {
        case .native: return "Apple å…§å»ºè½å¯« (æ¥µé€Ÿ âš¡ï¸)"
        case .base: return "Whisper Base (å¹³è¡¡ âš–ï¸)"
        case .openaiLargeV3Turbo_626MB: return "Turbo (ç²¾æº–/æ…¢ ğŸ¢)"
        }
    }
    
    // åˆ¤æ–·æ˜¯å¦ç‚º Apple å¼•æ“
    var isNative: Bool { return self == .native }
}

@MainActor
class STTService: ObservableObject {
    // MARK: - Published States
    @Published var isModelLoading = false
    @Published var statusMessage = "ç­‰å¾…é¸æ“‡éŠæˆ²..."
    
    @Published var currentModel: WhisperModel = .native // é è¨­å…ˆç”¨ Native (æœ€å¿«)
    
    // MARK: - Whisper å¼•æ“è®Šæ•¸
    private var pipe: WhisperKit?
    private var audioRecorder: AVAudioRecorder?
    private var audioFilename: URL?
    private var currentKeywords: [String] = []
    
    // MARK: - Apple Native å¼•æ“è®Šæ•¸
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-TW"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private var audioEngine: AVAudioEngine? // æ”¹æˆ Optional var
    private var nativeLastTranscription: String?
    // ğŸ‘‡ [æ–°å¢] ç”¨ä¾†æš«å­˜ç­‰å¾…ä¸­çš„ Continuation
    private var recognitionContinuation: CheckedContinuation<String?, Never>?
    // MARK: - æ¨¡å‹åˆ‡æ›èˆ‡è¨­å®š
    
    func setupWhisper(keywords: [String]) async {
        self.currentKeywords = keywords
        
        if currentModel.isNative {
            // A. Apple Native æ¨¡å¼
            print("ğŸ åˆ‡æ›è‡³ Apple Native å¼•æ“")
            // é‡‹æ”¾ Whisper è¨˜æ†¶é«”
            pipe = nil
            
            SFSpeechRecognizer.requestAuthorization { authStatus in
                DispatchQueue.main.async {
                    switch authStatus {
                    case .authorized:
                        self.statusMessage = "Apple è½å¯«å°±ç·’"
                    default:
                        self.statusMessage = "ç„¡è½å¯«æ¬Šé™"
                    }
                }
            }
            
        } else {
            // B. WhisperKit æ¨¡å¼
            print("ğŸ¤– åˆ‡æ›è‡³ Whisper å¼•æ“: \(currentModel.rawValue)")
            
            // å¦‚æœå·²ç¶“è¼‰å…¥åŒæ¬¾æ¨¡å‹ï¼Œå°±è·³é
            if pipe != nil && pipe?.modelState == .loaded {
                 // é€™è£¡ç°¡åŒ–åˆ¤æ–·ï¼Œå¯¦éš›å¯æ›´åš´è¬¹
                 // è‹¥æƒ³å¼·åˆ¶åˆ‡æ› Base/Turboï¼Œå»ºè­°é‚„æ˜¯é‡æ–° load
            }
            
            self.isModelLoading = true
            self.statusMessage = "è¼‰å…¥æ¨¡å‹: \(currentModel.displayName)..."
            
            do {
                // é‡‹æ”¾èˆŠæ¨¡å‹
                pipe = nil
                
                // ä¸‹è¼‰ä¸¦è¼‰å…¥æ–°æ¨¡å‹
                pipe = try await WhisperKit(model: currentModel.rawValue, download: true)
                
                // Warmup
                self.statusMessage = "æ­£åœ¨ç†±èº«..."
                try? await pipe?.transcribe(audioArray: [Float](repeating: 0, count: 16000))
                
                self.isModelLoading = false
                self.statusMessage = "Whisper å°±ç·’"
                print("âœ… Whisper æ¨¡å‹è¼‰å…¥å®Œæˆ")
            } catch {
                self.statusMessage = "è¼‰å…¥å¤±æ•—: \(error.localizedDescription)"
                print("âŒ Whisper Error: \(error)")
                self.isModelLoading = false
            }
        }
    }
    
    func switchModel(to newModel: WhisperModel) {
        if newModel == currentModel { return }
        print("ğŸ”„ åˆ‡æ›æ¨¡å‹è‡³: \(newModel.rawValue)")
        currentModel = newModel
        UserDefaults.standard.set(newModel.rawValue, forKey: "selected_whisper_model")
        
        // é€™è£¡ä¸éœ€ç«‹å³ resetModelï¼Œå› ç‚º MainViewModel æœƒå‘¼å« reloadModel æˆ– setupWhisper
        // ä½†ç‚ºäº†ä¿éšªï¼Œå…ˆæ¸…ç©ºç‹€æ…‹
        resetModel()
    }
    
    func resetModel() {
        // æ¸…ç©ºæ‰€æœ‰å¼•æ“ç‹€æ…‹
        pipe = nil
        stopNativeAudioEngine()
        audioRecorder?.stop()
        audioRecorder = nil
        print("ğŸ—‘ æ‰€æœ‰å¼•æ“è¨˜æ†¶é«”å·²é‡‹æ”¾")
    }
    
    // MARK: - éŒ„éŸ³å…¥å£ (è‡ªå‹•åˆ†æµ)
    
    func startRecording() async {
        if currentModel.isNative {
            await startNativeRecording()
        } else {
            await startWhisperRecording()
        }
    }
    /// å¼·åˆ¶é‡‹æ”¾éŒ„éŸ³å¼•æ“è³‡æº (è§£æ±º mDataByteSize 0 éŒ¯èª¤)
    func forceReleaseAudioResources() {
        print("ğŸ›¡ï¸ [STTService] å¤–éƒ¨å‘¼å«ï¼šå¼·åˆ¶é‡‹æ”¾éŸ³è¨Šè³‡æº")
        // å‘¼å«åŸæœ¬å…§éƒ¨çš„åœæ­¢é‚è¼¯
        stopNativeAudioEngine()
    }
    func stopAndTranscribe() async -> String? {
        if currentModel.isNative {
            return await stopNativeRecordingAndGetResult()
        } else {
            return await stopWhisperRecordingAndTranscribe()
        }
    }
    
    // MARK: - å¼•æ“ A: Apple Native å¯¦ä½œ
    
    private func startNativeRecording() async {
            print("ğŸ™ï¸ [Native] åˆå§‹åŒ–éŒ„éŸ³æµç¨‹...")
            
            // 1. ğŸ”¥ [é—œéµä¿®æ­£] å…ˆåŸ·è¡Œæ¸…ç†ï¼
            // å¿…é ˆåœ¨è¨­å®š Session ä¹‹å‰åŸ·è¡Œï¼Œå› ç‚ºé€™å‡½å¼è£¡é¢æœƒ setActive(false)
            stopNativeAudioEngine()
            
            // 2. è¨­å®š Session (ç¾åœ¨åŸ·è¡Œæ‰æ˜¯å°çš„ï¼Œæœƒé‡æ–° setActive(true))
            let session = AVAudioSession.sharedInstance()
            do {
                try session.setCategory(.playAndRecord, mode: .measurement, options: [.defaultToSpeaker, .allowBluetooth])
                try session.setActive(true, options: .notifyOthersOnDeactivation)
            } catch {
                print("âš ï¸ [STT] Session è¨­å®šå¤±æ•—: \(error)")
            }
            
            // 3. ğŸ”¥ [é‡å»ºå¼•æ“] å»ºç«‹å…¨æ–°çš„ AVAudioEngine
            // é€™æ˜¯è§£æ±º -66748 çš„æ ¸å¿ƒï¼šæ¯æ¬¡éŒ„éŸ³éƒ½ç”¨æ–°çš„å¼•æ“
            audioEngine = AVAudioEngine()
            // å»ºç«‹å€åŸŸè®Šæ•¸ä»¥æ–¹ä¾¿å¾ŒçºŒæ“ä½œ (Shadowing self.audioEngine)
            guard let audioEngine = audioEngine else {
                print("âŒ [Native] ç„¡æ³•å»ºç«‹ AudioEngine")
                return
            }
            
            // 4. æº–å‚™ Request
            recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
            guard let recognitionRequest = recognitionRequest else { return }
            recognitionRequest.shouldReportPartialResults = true
            
            // 5. è¨­å®š Input Node
            let inputNode = audioEngine.inputNode
            let recordingFormat = inputNode.outputFormat(forBus: 0)
            
            // å®‰è£ Tap
            inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
                recognitionRequest.append(buffer)
            }
            
            // 6. é–‹å§‹éŒ„éŸ³
            audioEngine.prepare()
            do {
                try audioEngine.start()
                self.statusMessage = "æ­£åœ¨è†è½ (Native)..."
                
                // 7. å•Ÿå‹•è¾¨è­˜ Task
                recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { [weak self] result, error in
                    guard let self = self else { return }
                    
                    if let result = result {
                        self.nativeLastTranscription = result.bestTranscription.formattedString
                        
                        if result.isFinal {
                            self.recognitionContinuation?.resume(returning: self.nativeLastTranscription)
                            self.recognitionContinuation = nil
                        }
                    }
                    
                    if let error = error {
                        // é€™è£¡ä¸éœ€è¦å‘¼å« stopNativeAudioEngineï¼Œå› ç‚ºå¯èƒ½æœƒè·Ÿå¤–éƒ¨çš„ stop è¡çª
                        // åªè¦ç¢ºä¿ Continuation æœ‰å›æ‡‰å³å¯
                        print("âš ï¸ [Native] è¾¨è­˜éç¨‹éŒ¯èª¤/çµæŸ: \(error.localizedDescription)")
                        self.recognitionContinuation?.resume(returning: self.nativeLastTranscription)
                        self.recognitionContinuation = nil
                    }
                }
                print("ğŸ™ï¸ [Native] éŒ„éŸ³å¼•æ“å•Ÿå‹•æˆåŠŸ")
            } catch {
                print("âŒ [Native] å•Ÿå‹•å¤±æ•—: \(error)")
            }
        }
    private func stopNativeAudioEngine() {
            // 1. éŠ·æ¯€å¼•æ“ (ç¶­æŒåŸæœ¬é‚è¼¯)
            if let engine = audioEngine {
                if engine.isRunning {
                    engine.stop()
                    engine.inputNode.removeTap(onBus: 0)
                    engine.reset()
                }
            }
            audioEngine = nil
            
            // 2. æ¸…ç† Request
            recognitionRequest?.endAudio()
            recognitionRequest = nil
            
            // 3. ğŸ”¥ [æ–°å¢] å¼·åˆ¶å–æ¶ˆè¾¨è­˜ä»»å‹™
            // é¿å… SFSpeechRecognitionTask åœ¨èƒŒæ™¯é‚„å’¬è‘—è³‡æº
            recognitionTask?.cancel()
            recognitionTask = nil
            
            // 4. âŒ [ç§»é™¤] ä¸è¦åŸ·è¡Œ setActive(false)ï¼
            // æˆ‘å€‘ä¿æŒ Session ç‚º Activeï¼Œè®“ MainViewModel ç›´æ¥åˆ‡æ› Category å³å¯ã€‚
            // é€™èƒ½é¿å… "connection invalidated" å°è‡´çš„ -66748 éŒ¯èª¤ã€‚
            print("ğŸ›¡ï¸ [STTService] å¼•æ“å·²éŠ·æ¯€ï¼ŒSession ä¿æŒ Active ç­‰å¾…åˆ‡æ›...")
        }
    
    private func stopNativeRecordingAndGetResult() async -> String? {
        // 1. å‘Šè¨´ç³»çµ±éŒ„éŸ³è³‡æ–™çµæŸäº†ï¼Œé€™æœƒè§¸ç™¼ recognitionTask é€²è¡Œæœ€å¾Œè™•ç†ä¸¦å›å‚³ isFinal
        recognitionRequest?.endAudio()
        
        // âŒ [ç§»é™¤] èˆŠçš„å¯«æ³•ï¼šä¸ç©©å®šçš„ç­‰å¾…
        // stopNativeAudioEngine()
        // try? await Task.sleep(nanoseconds: 200_000_000)
        // let text = nativeLastTranscription
        // ...
        
        // âœ… [æ–°å¯«æ³•] ä½¿ç”¨ Continuation å®‰å…¨ç­‰å¾…çµæœ
        let finalResult: String? = await withCheckedContinuation { continuation in
            // å„²å­˜é€™å€‹ continuationï¼Œè®“ startNativeRecording è£¡çš„é–‰åŒ…å¯ä»¥å‘¼å«å®ƒ
            self.recognitionContinuation = continuation
            
            // âš ï¸ [å®‰å…¨æ©Ÿåˆ¶] è¨­å®šä¸€å€‹ 2 ç§’çš„ Timeout
            // è¬ä¸€ Apple çš„ API æ²’æœ‰å›å‚³ isFinal ä¹Ÿä¸å ±éŒ¯ï¼Œæˆ‘å€‘ä¸èƒ½è®“ App å¡æ­»
            Task {
                try? await Task.sleep(nanoseconds: 2_000_000_000) // 2ç§’
                if self.recognitionContinuation != nil {
                    print("âš ï¸ [Native] ç­‰å¾…çµæœé€¾æ™‚ï¼Œå¼·åˆ¶å›å‚³ç›®å‰çµæœ")
                    self.recognitionContinuation?.resume(returning: self.nativeLastTranscription)
                    self.recognitionContinuation = nil
                }
            }
        }
        
        // 2. ç¢ºä¿ Audio Engine é—œé–‰
        stopNativeAudioEngine()
        
        // 3. é‡ç½®ç‹€æ…‹
        nativeLastTranscription = nil
        recognitionTask = nil
        
        print("ğŸ [Native æœ€çµ‚çµæœ]: \(finalResult ?? "nil")")
        return (finalResult?.isEmpty ?? true) ? nil : finalResult
    }
    
    // MARK: - å¼•æ“ B: WhisperKit å¯¦ä½œ
    
    private func startWhisperRecording() async {
        print("ğŸ™ï¸ [Whisper] æº–å‚™éŒ„éŸ³...")
        
        let session = AVAudioSession.sharedInstance()
        try? session.setActive(false)
        // Whisper æ¯”è¼ƒé©åˆ videoRecording æ¨¡å¼ (Raw Audio)
        try? session.setCategory(.playAndRecord, mode: .videoRecording, options: [.defaultToSpeaker, .allowBluetooth])
        try? session.setActive(true)
        
        let url = FileManager.default.temporaryDirectory.appendingPathComponent("whisper_input.wav")
        self.audioFilename = url
        
        let settings: [String: Any] = [
            AVFormatIDKey: kAudioFormatLinearPCM,
            AVSampleRateKey: 16000,
            AVNumberOfChannelsKey: 1,
            AVEncoderAudioQualityKey: AVAudioQuality.high.rawValue
        ]
        
        do {
            audioRecorder = try AVAudioRecorder(url: url, settings: settings)
            audioRecorder?.isMeteringEnabled = true
            audioRecorder?.record()
            print("ğŸ™ï¸ [Whisper] éŒ„éŸ³é–‹å§‹")
        } catch {
            print("âŒ [Whisper] éŒ„éŸ³å¤±æ•—: \(error)")
        }
    }
    
    private func stopWhisperRecordingAndTranscribe() async -> String? {
        // 1. åœæ­¢
        audioRecorder?.stop()
        audioRecorder = nil
        
        guard let pipe = pipe, let url = audioFilename else { return nil }
        
        // 2. æª¢æŸ¥æª”æ¡ˆ
        if !FileManager.default.fileExists(atPath: url.path) { return nil }
        
        // 3. Prompt
        // é€™è£¡å°‡ Native é¸æ“‡çš„ keywords è½‰ç‚º Prompt
        let promptText = "ç¹é«”ä¸­æ–‡æ¡ŒéŠå°è©±ã€‚é—œéµè©ï¼š\(currentKeywords.joined(separator: ", "))"
        
        // Encode prompt tokens (ç°¡åŒ–ç‰ˆ)
        let promptTokens = pipe.tokenizer?.encode(text: promptText).filter { $0 < (pipe.tokenizer?.specialTokens.specialTokenBegin ?? 50257) }
        
        // 4. Decode Options
        // æ‚¨è¦æ±‚çš„ Base æ¸¬è©¦ï¼šä½¿ç”¨è¼ƒæ­£å¸¸çš„åƒæ•¸
        let options = DecodingOptions(
            language: "zh",
            temperature: 0.0,
            promptTokens: promptTokens, // Prompt æ”¾åœ¨é€™
            compressionRatioThreshold: 2.4,
            logProbThreshold: -2.0,     // ä¸å†ä½¿ç”¨ -100ï¼Œæ”¹å›æ­£å¸¸å€¼
            noSpeechThreshold: 0.4      // é™ä½é–€æª»
        )
        
        print("ğŸ“ [Whisper] é–‹å§‹æ¨è«–...")
        self.statusMessage = "Whisper æ€è€ƒä¸­..."
        
        do {
            let result = try await pipe.transcribe(audioPath: url.path, decodeOptions: options)
            let text = result.first?.text.trimmingCharacters(in: .whitespacesAndNewlines)
            print("ğŸ¤– [Whisper çµæœ]: \(text ?? "nil")")
            return (text?.isEmpty ?? true) ? nil : text
        } catch {
            print("âŒ [Whisper] æ¨è«–å¤±æ•—: \(error)")
            return nil
        }
    }
}
