import Foundation
import AVFoundation
import Speech  // ğŸ‘ˆ å¼•å…¥ Apple èªéŸ³æ¡†æ¶
import WhisperKit
import Combine

// MARK: - æ¨¡å‹å®šç¾© (é›™å¼•æ“æ•´åˆ)
enum WhisperModel: String, CaseIterable, Identifiable {
    // ğŸ Apple å…§å»º
    case native = "native_apple"
    
    // ğŸ¤– OpenAI Whisper ç³»åˆ—
    case base = "base"                                      // User æŒ‡å®šæ¸¬è©¦
    case openaiLargeV3Turbo_626MB = "openai_whisper-large-v3-v20240930_626MB" // åŸæœ¬çš„ Turbo
    
    var id: String { self.rawValue }
    
    var displayName: String {
        switch self {
        case .native: return "Apple å…§å»ºè½å¯« (æ¥µé€Ÿ âš¡ï¸)"
        case .base: return "Whisper Base (å¹³è¡¡ âš–ï¸)"
        case .openaiLargeV3Turbo_626MB: return "Turbo (ç²¾æº–/æ…¢ ğŸ¢)"
        }
    }
    
    // åˆ¤æ–·æ˜¯å¦ç‚º Apple å¼•æ“
    var isNative: Bool { return self == .native }
}

@MainActor
class STTService: ObservableObject {
    // MARK: - Published States
    @Published var isModelLoading = false
    @Published var statusMessage = "ç­‰å¾…é¸æ“‡éŠæˆ²..."
    
    @Published var currentModel: WhisperModel = .native // é è¨­å…ˆç”¨ Native (æœ€å¿«)
    
    // MARK: - Whisper å¼•æ“è®Šæ•¸
    private var pipe: WhisperKit?
    private var audioRecorder: AVAudioRecorder?
    private var audioFilename: URL?
    private var currentKeywords: [String] = []
    
    // MARK: - Apple Native å¼•æ“è®Šæ•¸
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-TW"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    private var nativeLastTranscription: String?
    // ğŸ‘‡ [æ–°å¢] ç”¨ä¾†æš«å­˜ç­‰å¾…ä¸­çš„ Continuation
    private var recognitionContinuation: CheckedContinuation<String?, Never>?
    // MARK: - æ¨¡å‹åˆ‡æ›èˆ‡è¨­å®š
    
    func setupWhisper(keywords: [String]) async {
        self.currentKeywords = keywords
        
        if currentModel.isNative {
            // A. Apple Native æ¨¡å¼
            print("ğŸ åˆ‡æ›è‡³ Apple Native å¼•æ“")
            // é‡‹æ”¾ Whisper è¨˜æ†¶é«”
            pipe = nil
            
            SFSpeechRecognizer.requestAuthorization { authStatus in
                DispatchQueue.main.async {
                    switch authStatus {
                    case .authorized:
                        self.statusMessage = "Apple è½å¯«å°±ç·’"
                    default:
                        self.statusMessage = "ç„¡è½å¯«æ¬Šé™"
                    }
                }
            }
            
        } else {
            // B. WhisperKit æ¨¡å¼
            print("ğŸ¤– åˆ‡æ›è‡³ Whisper å¼•æ“: \(currentModel.rawValue)")
            
            // å¦‚æœå·²ç¶“è¼‰å…¥åŒæ¬¾æ¨¡å‹ï¼Œå°±è·³é
            if pipe != nil && pipe?.modelState == .loaded {
                 // é€™è£¡ç°¡åŒ–åˆ¤æ–·ï¼Œå¯¦éš›å¯æ›´åš´è¬¹
                 // è‹¥æƒ³å¼·åˆ¶åˆ‡æ› Base/Turboï¼Œå»ºè­°é‚„æ˜¯é‡æ–° load
            }
            
            self.isModelLoading = true
            self.statusMessage = "è¼‰å…¥æ¨¡å‹: \(currentModel.displayName)..."
            
            do {
                // é‡‹æ”¾èˆŠæ¨¡å‹
                pipe = nil
                
                // ä¸‹è¼‰ä¸¦è¼‰å…¥æ–°æ¨¡å‹
                pipe = try await WhisperKit(model: currentModel.rawValue, download: true)
                
                // Warmup
                self.statusMessage = "æ­£åœ¨ç†±èº«..."
                try? await pipe?.transcribe(audioArray: [Float](repeating: 0, count: 16000))
                
                self.isModelLoading = false
                self.statusMessage = "Whisper å°±ç·’"
                print("âœ… Whisper æ¨¡å‹è¼‰å…¥å®Œæˆ")
            } catch {
                self.statusMessage = "è¼‰å…¥å¤±æ•—: \(error.localizedDescription)"
                print("âŒ Whisper Error: \(error)")
                self.isModelLoading = false
            }
        }
    }
    
    func switchModel(to newModel: WhisperModel) {
        if newModel == currentModel { return }
        print("ğŸ”„ åˆ‡æ›æ¨¡å‹è‡³: \(newModel.rawValue)")
        currentModel = newModel
        UserDefaults.standard.set(newModel.rawValue, forKey: "selected_whisper_model")
        
        // é€™è£¡ä¸éœ€ç«‹å³ resetModelï¼Œå› ç‚º MainViewModel æœƒå‘¼å« reloadModel æˆ– setupWhisper
        // ä½†ç‚ºäº†ä¿éšªï¼Œå…ˆæ¸…ç©ºç‹€æ…‹
        resetModel()
    }
    
    func resetModel() {
        // æ¸…ç©ºæ‰€æœ‰å¼•æ“ç‹€æ…‹
        pipe = nil
        stopNativeAudioEngine()
        audioRecorder?.stop()
        audioRecorder = nil
        print("ğŸ—‘ æ‰€æœ‰å¼•æ“è¨˜æ†¶é«”å·²é‡‹æ”¾")
    }
    
    // MARK: - éŒ„éŸ³å…¥å£ (è‡ªå‹•åˆ†æµ)
    
    func startRecording() async {
        if currentModel.isNative {
            await startNativeRecording()
        } else {
            await startWhisperRecording()
        }
    }
    
    func stopAndTranscribe() async -> String? {
        if currentModel.isNative {
            return await stopNativeRecordingAndGetResult()
        } else {
            return await stopWhisperRecordingAndTranscribe()
        }
    }
    
    // MARK: - å¼•æ“ A: Apple Native å¯¦ä½œ
    
    private func startNativeRecording() async {
        let session = AVAudioSession.sharedInstance()
        do {
            // ğŸ”¥ [ä¿®æ”¹] ç›´æ¥åˆ‡æ› Mode ç‚º measurement (é©åˆèªéŸ³è¾¨è­˜)ï¼Œä¿æŒ Active
            // ç§»é™¤ setActive(false) ä»¥é¿å…ç¡¬é«”é‡å•Ÿå»¶é²
            try session.setCategory(.playAndRecord, mode: .measurement, options: [.defaultToSpeaker, .allowBluetooth])
            try session.setActive(true)
            } catch {
                print("âš ï¸ [STT] Session è¨­å®šå¤±æ•—: \(error)")
            }
        
        // 2. æº–å‚™ Request
        stopNativeAudioEngine() // ç¢ºä¿ä¹¾æ·¨
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else { return }
        recognitionRequest.shouldReportPartialResults = true
        
        // 3. è¨­å®š Input Node
        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
            recognitionRequest.append(buffer)
        }
        
        // 4. é–‹å§‹
        audioEngine.prepare()
        do {
            try audioEngine.start()
            self.statusMessage = "æ­£åœ¨è†è½ (Native)..."
            
            // å•Ÿå‹• Task
            // ğŸ‘‡ [ä¿®æ”¹] é€™è£¡çš„é–‰åŒ…å…§å®¹è¦æ›´æ–°
                    recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { [weak self] result, error in
                        guard let self = self else { return }
                        
                        if let result = result {
                            // 1. å¯¦æ™‚æ›´æ–°æ–‡å­—
                            self.nativeLastTranscription = result.bestTranscription.formattedString
                            
                            // 2. âœ… [æ–°å¢] å¦‚æœæ˜¯æœ€çµ‚çµæœ (isFinal)ï¼Œå°±å–šé†’ç­‰å¾…ä¸­çš„ Continuation
                            if result.isFinal {
                                self.recognitionContinuation?.resume(returning: self.nativeLastTranscription)
                                self.recognitionContinuation = nil
                            }
                        }
                        
                        if let error = error {
                            self.stopNativeAudioEngine()
                            
                            // 3. âœ… [æ–°å¢] å¦‚æœç™¼ç”ŸéŒ¯èª¤ï¼Œä¹Ÿè¦å–šé†’ Continuation (å›å‚³ç›®å‰çš„çµæœæˆ– nil)
                            // é€™æ¨£ stopNativeRecordingAndGetResult å°±ä¸æœƒä¸€ç›´å¡ä½
                            self.recognitionContinuation?.resume(returning: self.nativeLastTranscription)
                            self.recognitionContinuation = nil
                        }
                    }
                    print("ğŸ™ï¸ [Native] é–‹å§‹éŒ„éŸ³")
                } catch {
                    print("âŒ [Native] å•Ÿå‹•å¤±æ•—: \(error)")
                }
            }
    
    private func stopNativeAudioEngine() {
        if audioEngine.isRunning {
           audioEngine.stop()
           audioEngine.inputNode.removeTap(onBus: 0)
        // ğŸ”¥ [æ–°å¢] å¼·åˆ¶é‡ç½®å¼•æ“ï¼Œé‡‹æ”¾ç¡¬é«”è³‡æºï¼Œè§£æ±º TTS -66748 éŒ¯èª¤
            audioEngine.reset()
        }
        recognitionRequest?.endAudio()
        recognitionRequest = nil
        // recognitionTask ä¸ cancelï¼Œä¿ç•™çµæœ
     }
    
    private func stopNativeRecordingAndGetResult() async -> String? {
        // 1. å‘Šè¨´ç³»çµ±éŒ„éŸ³è³‡æ–™çµæŸäº†ï¼Œé€™æœƒè§¸ç™¼ recognitionTask é€²è¡Œæœ€å¾Œè™•ç†ä¸¦å›å‚³ isFinal
        recognitionRequest?.endAudio()
        
        // âŒ [ç§»é™¤] èˆŠçš„å¯«æ³•ï¼šä¸ç©©å®šçš„ç­‰å¾…
        // stopNativeAudioEngine()
        // try? await Task.sleep(nanoseconds: 200_000_000)
        // let text = nativeLastTranscription
        // ...
        
        // âœ… [æ–°å¯«æ³•] ä½¿ç”¨ Continuation å®‰å…¨ç­‰å¾…çµæœ
        let finalResult: String? = await withCheckedContinuation { continuation in
            // å„²å­˜é€™å€‹ continuationï¼Œè®“ startNativeRecording è£¡çš„é–‰åŒ…å¯ä»¥å‘¼å«å®ƒ
            self.recognitionContinuation = continuation
            
            // âš ï¸ [å®‰å…¨æ©Ÿåˆ¶] è¨­å®šä¸€å€‹ 2 ç§’çš„ Timeout
            // è¬ä¸€ Apple çš„ API æ²’æœ‰å›å‚³ isFinal ä¹Ÿä¸å ±éŒ¯ï¼Œæˆ‘å€‘ä¸èƒ½è®“ App å¡æ­»
            Task {
                try? await Task.sleep(nanoseconds: 2_000_000_000) // 2ç§’
                if self.recognitionContinuation != nil {
                    print("âš ï¸ [Native] ç­‰å¾…çµæœé€¾æ™‚ï¼Œå¼·åˆ¶å›å‚³ç›®å‰çµæœ")
                    self.recognitionContinuation?.resume(returning: self.nativeLastTranscription)
                    self.recognitionContinuation = nil
                }
            }
        }
        
        // 2. ç¢ºä¿ Audio Engine é—œé–‰
        stopNativeAudioEngine()
        
        // 3. é‡ç½®ç‹€æ…‹
        nativeLastTranscription = nil
        recognitionTask = nil
        
        print("ğŸ [Native æœ€çµ‚çµæœ]: \(finalResult ?? "nil")")
        return (finalResult?.isEmpty ?? true) ? nil : finalResult
    }
    
    // MARK: - å¼•æ“ B: WhisperKit å¯¦ä½œ
    
    private func startWhisperRecording() async {
        print("ğŸ™ï¸ [Whisper] æº–å‚™éŒ„éŸ³...")
        
        let session = AVAudioSession.sharedInstance()
        try? session.setActive(false)
        // Whisper æ¯”è¼ƒé©åˆ videoRecording æ¨¡å¼ (Raw Audio)
        try? session.setCategory(.playAndRecord, mode: .videoRecording, options: [.defaultToSpeaker, .allowBluetooth])
        try? session.setActive(true)
        
        let url = FileManager.default.temporaryDirectory.appendingPathComponent("whisper_input.wav")
        self.audioFilename = url
        
        let settings: [String: Any] = [
            AVFormatIDKey: kAudioFormatLinearPCM,
            AVSampleRateKey: 16000,
            AVNumberOfChannelsKey: 1,
            AVEncoderAudioQualityKey: AVAudioQuality.high.rawValue
        ]
        
        do {
            audioRecorder = try AVAudioRecorder(url: url, settings: settings)
            audioRecorder?.isMeteringEnabled = true
            audioRecorder?.record()
            print("ğŸ™ï¸ [Whisper] éŒ„éŸ³é–‹å§‹")
        } catch {
            print("âŒ [Whisper] éŒ„éŸ³å¤±æ•—: \(error)")
        }
    }
    
    private func stopWhisperRecordingAndTranscribe() async -> String? {
        // 1. åœæ­¢
        audioRecorder?.stop()
        audioRecorder = nil
        
        guard let pipe = pipe, let url = audioFilename else { return nil }
        
        // 2. æª¢æŸ¥æª”æ¡ˆ
        if !FileManager.default.fileExists(atPath: url.path) { return nil }
        
        // 3. Prompt
        // é€™è£¡å°‡ Native é¸æ“‡çš„ keywords è½‰ç‚º Prompt
        let promptText = "ç¹é«”ä¸­æ–‡æ¡ŒéŠå°è©±ã€‚é—œéµè©ï¼š\(currentKeywords.joined(separator: ", "))"
        
        // Encode prompt tokens (ç°¡åŒ–ç‰ˆ)
        let promptTokens = pipe.tokenizer?.encode(text: promptText).filter { $0 < (pipe.tokenizer?.specialTokens.specialTokenBegin ?? 50257) }
        
        // 4. Decode Options
        // æ‚¨è¦æ±‚çš„ Base æ¸¬è©¦ï¼šä½¿ç”¨è¼ƒæ­£å¸¸çš„åƒæ•¸
        let options = DecodingOptions(
            language: "zh",
            temperature: 0.0,
            promptTokens: promptTokens, // Prompt æ”¾åœ¨é€™
            compressionRatioThreshold: 2.4,
            logProbThreshold: -2.0,     // ä¸å†ä½¿ç”¨ -100ï¼Œæ”¹å›æ­£å¸¸å€¼
            noSpeechThreshold: 0.4      // é™ä½é–€æª»
        )
        
        print("ğŸ“ [Whisper] é–‹å§‹æ¨è«–...")
        self.statusMessage = "Whisper æ€è€ƒä¸­..."
        
        do {
            let result = try await pipe.transcribe(audioPath: url.path, decodeOptions: options)
            let text = result.first?.text.trimmingCharacters(in: .whitespacesAndNewlines)
            print("ğŸ¤– [Whisper çµæœ]: \(text ?? "nil")")
            return (text?.isEmpty ?? true) ? nil : text
        } catch {
            print("âŒ [Whisper] æ¨è«–å¤±æ•—: \(error)")
            return nil
        }
    }
}
