import SwiftUI
import AVFoundation
import Speech
import Combine // ç¢ºä¿å¼•å…¥ Combine ä»¥æ”¯æ´ ObservableObject

// MARK: - é©—è­‰ç”¨ ViewModel
class AudioTestViewModel: ObservableObject {
    @Published var status = "æº–å‚™å°±ç·’"
    @Published var recognizedText = ""
    @Published var isRecording = false
    
    // ğŸ¤ éŒ„éŸ³ç›¸é—œ
    private var audioEngine: AVAudioEngine?
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-TW"))
    
    // ğŸ”Š æ’­æ”¾ç›¸é—œ
    // æ¯æ¬¡æ’­æ”¾éƒ½ä½¿ç”¨æ–°çš„ Synthesizer ä»¥é¿å…èˆŠå¯¦é«”æå£
    private var currentSynthesizer: AVSpeechSynthesizer?
    
    // MARK: - å‹•ä½œ 1: é–‹å§‹éŒ„éŸ³
    func startRecording() {
        // ä½¿ç”¨ Task ç¢ºä¿åœ¨èƒŒæ™¯åŸ·è¡Œï¼Œä¸¦å›åˆ° MainActor æ›´æ–° UI
        Task { @MainActor in
            print("\nğŸ™ï¸ ======== [å‹•ä½œ: é–‹å§‹éŒ„éŸ³] ========")
            
            // A. æ¸…ç†èˆŠæˆ°å ´
            cleanupEngine()
            
            // B. é‡ç½® Session (å…ˆé—œå†é–‹ï¼Œç¢ºä¿ä¹¾æ·¨)
            let session = AVAudioSession.sharedInstance()
            do {
                print("   1ï¸âƒ£ [Session] æº–å‚™éŒ„éŸ³ç’°å¢ƒ...")
                // å…ˆå˜—è©¦è§£é™¤é–å®š (é›–ä¸ä¸€å®šå¿…è¦ï¼Œä½†ä¿éšª)
                try? session.setActive(false)
                
                // è¨­å®šç‚ºéŒ„éŸ³æ¨¡å¼
                try session.setCategory(.playAndRecord, mode: .measurement, options: [.defaultToSpeaker, .allowBluetooth])
                try session.setActive(true, options: .notifyOthersOnDeactivation)
                print("      âœ… Session Active (PlayAndRecord)")
            } catch {
                print("      âŒ Session Error: \(error)")
                self.status = "Session Error"
                return
            }
            
            // C. å»ºç«‹å…¨æ–°å¼•æ“
            print("   2ï¸âƒ£ [Engine] å»ºç«‹å…¨æ–° AVAudioEngine")
            let newEngine = AVAudioEngine()
            self.audioEngine = newEngine
            
            self.recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
            guard let recognitionRequest = self.recognitionRequest else { return }
            recognitionRequest.shouldReportPartialResults = true
            
            let inputNode = newEngine.inputNode
            let recordingFormat = inputNode.outputFormat(forBus: 0)
            print("      â„¹ï¸ Input Format: \(recordingFormat)")
            
            inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
                self.recognitionRequest?.append(buffer)
            }
            
            newEngine.prepare()
            
            do {
                try newEngine.start()
                self.isRecording = true
                self.status = "æ­£åœ¨éŒ„éŸ³...è«‹èªªè©±"
                self.recognizedText = ""
                print("   3ï¸âƒ£ [Engine] å•Ÿå‹•æˆåŠŸ (Running)")
                
                self.recognitionTask = self.speechRecognizer?.recognitionTask(with: recognitionRequest) { [weak self] result, error in
                    if let result = result {
                        self?.recognizedText = result.bestTranscription.formattedString
                    }
                    if let error = error {
                        print("      âš ï¸ è¾¨è­˜çµæŸ/éŒ¯èª¤: \(error.localizedDescription)")
                    }
                }
            } catch {
                print("   âŒ Engine Start Error: \(error)")
                self.status = "Engine Start Error"
            }
        }
    }
    
    // MARK: - å‹•ä½œ 2: åœæ­¢ä¸¦è¤‡è®€ (æ ¸å¿ƒä¿®æ”¹)
    func stopAndRepeat() {
        Task { @MainActor in
            print("\nğŸ›‘ ======== [å‹•ä½œ: åœæ­¢ä¸¦è¤‡è®€] ========")
            
            // 1. å¾¹åº•éŠ·æ¯€å¼•æ“
            print("   1ï¸âƒ£ [Cleanup] éŠ·æ¯€å¼•æ“...")
            if let engine = audioEngine {
                if engine.isRunning {
                    engine.stop()
                }
                engine.inputNode.removeTap(onBus: 0)
                engine.reset()
            }
            audioEngine = nil
            print("      ğŸ”¥ Engine set to NIL")
            
            recognitionRequest?.endAudio()
            recognitionRequest = nil
            recognitionTask?.cancel()
            recognitionTask = nil
            
            self.isRecording = false
            self.status = "è™•ç†ä¸­ (ç¡¬é«”é‡‹æ”¾)..."
            
            // ğŸ”¥ [é—œéµä¿®æ­£] åŠ å…¥ç·©è¡æ™‚é–“ï¼Œè®“ç¡¬é«”å¾¹åº•é‡‹æ”¾éº¥å…‹é¢¨
            // é€™èƒ½é˜²æ­¢ -66748 éŒ¯èª¤ (Connection Invalidated)
            print("   â³ [Wait] ç­‰å¾… 0.5 ç§’è®“ç¡¬é«”é‡‹æ”¾...")
            try? await Task.sleep(nanoseconds: 500_000_000) // 0.5ç§’
            
            // 2. åˆ‡æ› Session è‡³æ’­æ”¾æ¨¡å¼
            let session = AVAudioSession.sharedInstance()
            do {
                print("   2ï¸âƒ£ [Session] åˆ‡æ›è‡³ .playback (ç´”æ’­æ”¾)")
                
                // A. å…ˆ Deactivate (æ›æ–·é›»è©±) - è§£æ±ºè³‡æºä½”ç”¨
                try? session.setActive(false)
                
                // B. è¨­å®šç‚ºç´”æ’­æ”¾ (é€™æœƒè®“ç³»çµ±å°‡è·¯ç”±å°å‘å–‡å­ï¼Œä¸¦åˆ‡æ–·éº¥å…‹é¢¨é€£çµ)
                try session.setCategory(.playback, mode: .default, options: [])
                try session.setActive(true, options: .notifyOthersOnDeactivation)
                
                print("      âœ… Session Active (.playback)")
            } catch {
                print("      âŒ Session Switch Error: \(error)")
            }
            
            self.status = "æº–å‚™æ’­æ”¾..."
            
            // 3. æ’­æ”¾
            let textToSpeak = self.recognizedText.isEmpty ? "æ²’æœ‰è½åˆ°è²éŸ³" : self.recognizedText
            self.speak(text: textToSpeak)
        }
    }
    
    private func speak(text: String) {
        print("\nğŸ”Š ======== [å‹•ä½œ: TTS æ’­æ”¾] ========")
        print("   1ï¸âƒ£ [Synthesizer] å»ºç«‹å…¨æ–°å¯¦é«”")
        
        // æ¯æ¬¡éƒ½å»ºç«‹æ–°çš„ Synthesizerï¼Œç¢ºä¿æ²’æœ‰èˆŠçš„ Audio Unit æ®˜ç•™
        let newSynthesizer = AVSpeechSynthesizer()
        currentSynthesizer = newSynthesizer
        
        let utterance = AVSpeechUtterance(string: text)
        utterance.voice = AVSpeechSynthesisVoice(language: "zh-TW")
        utterance.rate = 0.5
        
        print("   2ï¸âƒ£ [Speak] å‘¼å« speak")
        newSynthesizer.speak(utterance)
        status = "æ­£åœ¨æ’­æ”¾: \(text)"
    }
    
    private func cleanupEngine() {
        print("   ğŸ§¹ [Cleanup] æ¸…ç†æ®˜ç•™å¼•æ“...")
        audioEngine?.stop()
        audioEngine = nil
        recognitionTask?.cancel()
        recognitionTask = nil
    }
}

// MARK: - é©—è­‰ç”¨ View
struct AudioTestView: View {
    @StateObject var vm = AudioTestViewModel()
    
    var body: some View {
        VStack(spacing: 30) {
            Text("Audio Crash é©—è­‰å™¨")
                .font(.largeTitle)
                .bold()
                .padding(.top)
            
            // Console Log æç¤º
            Text("è«‹è§€å¯Ÿ Xcode Console çš„è©³ç´° Log")
                .font(.caption)
                .foregroundColor(.gray)
            
            Divider()
            
            Text(vm.status)
                .font(.headline)
                .foregroundColor(.blue)
                .padding()
            
            Text(vm.recognizedText.isEmpty ? "(ç­‰å¾…èªéŸ³è¼¸å…¥...)" : vm.recognizedText)
                .font(.title2)
                .padding()
                .frame(maxWidth: .infinity)
                .frame(height: 150)
                .background(Color.gray.opacity(0.1))
                .cornerRadius(10)
                .padding(.horizontal)
            
            Button(action: {
                if vm.isRecording {
                    vm.stopAndRepeat()
                } else {
                    vm.startRecording()
                }
            }) {
                VStack {
                    Image(systemName: vm.isRecording ? "stop.circle.fill" : "mic.circle.fill")
                        .resizable()
                        .frame(width: 80, height: 80)
                        .foregroundColor(vm.isRecording ? .red : .blue)
                    
                    Text(vm.isRecording ? "åœæ­¢ä¸¦è¤‡è®€" : "é–‹å§‹éŒ„éŸ³")
                        .font(.headline)
                        .foregroundColor(.primary)
                }
            }
            
            Spacer()
            
            Text("é©—è­‰é‡é»ï¼š\n1. éŒ„éŸ³å¾Œç­‰å¾… 0.5s\n2. è§€å¯Ÿ Log æ˜¯å¦æˆåŠŸåˆ‡æ›ç‚º .playback\n3. å¿…é ˆè½åˆ°è²éŸ³")
                .font(.caption)
                .multilineTextAlignment(.center)
                .foregroundColor(.secondary)
                .padding(.bottom)
        }
    }
}
