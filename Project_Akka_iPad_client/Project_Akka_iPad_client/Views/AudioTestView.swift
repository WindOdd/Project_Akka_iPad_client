import SwiftUI
import Speech
import AVFoundation
import Combine
// MARK: - é©—è­‰ç”¨ ViewModel (æœ€çµ‚æ ¸å½ˆç´šä¿®å¾©ï¼šå¼·åˆ¶é‡å»º Synthesizer)
class AudioTestViewModel: NSObject, ObservableObject, AVSpeechSynthesizerDelegate {
    @Published var recognizedText = ""
    @Published var isRecording = false
    @Published var statusMessage = "æº–å‚™å°±ç·’"
    
    // æ ¸å¿ƒå…ƒä»¶
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-TW"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    
    // ğŸ”¥ é—œéµ 1: å…©å€‹éƒ½å¿…é ˆæ˜¯ varï¼Œå› ç‚ºéƒ½è¦é‡å»º
    // audioEngine: éŒ„éŸ³çµæŸå¾Œé‡å»ºï¼Œç¢ºä¿ Input Node ä¹¾æ·¨
    // speechSynthesizer: æ’­æ”¾å‰é‡å»ºï¼Œç¢ºä¿æ‹¿åˆ°æ–°çš„ Connection ID (è§£æ±º -66748)
    private var audioEngine = AVAudioEngine()
    private var speechSynthesizer = AVSpeechSynthesizer()
    
    override init() {
        super.init()
        speechSynthesizer.delegate = self
    }
    
    // MARK: - éŒ„éŸ³åŠŸèƒ½
    
    func toggleRecording() {
        if isRecording {
            stopRecording()
        } else {
            startRecording()
        }
    }
    
    func startRecording() {
        // 1. ç¢ºä¿ TTS é–‰å˜´
        if speechSynthesizer.isSpeaking {
            speechSynthesizer.stopSpeaking(at: .immediate)
        }
        
        // 2. æ¸…ç†èˆŠä»»å‹™
        if recognitionTask != nil {
            recognitionTask?.cancel()
            recognitionTask = nil
        }
        
        // 3. è¨­å®š Session (.record æ¨¡å¼)
        // ä½¿ç”¨ .record æ¨¡å¼æ˜¯æœ€å–®ç´”çš„ï¼Œå®ƒå‘Šè¨´ç³»çµ±ã€Œæˆ‘ç¾åœ¨åªè¦éº¥å…‹é¢¨ã€
        let audioSession = AVAudioSession.sharedInstance()
        do {
            try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        } catch {
            statusMessage = "Session è¨­å®šå¤±æ•—: \(error.localizedDescription)"
            return
        }
        
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            statusMessage = "ç„¡æ³•å»ºç«‹ Request"
            return
        }
        recognitionRequest.shouldReportPartialResults = true
        
        // 4. è¨­å®š Input
        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        
        // 5. å»ºç«‹ Task
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            guard let self = self else { return }
            var isFinal = false
            
            if let result = result {
                DispatchQueue.main.async {
                    self.recognizedText = result.bestTranscription.formattedString
                    print("è¾¨è­˜ä¸­ï¼š\(self.recognizedText)")
                }
                isFinal = result.isFinal
            }
            
            // ğŸ”¥ çµæŸæˆ–éŒ¯èª¤è™•ç†é‚è¼¯
            if error != nil || isFinal {
                // A. åœæ­¢å¼•æ“
                self.audioEngine.stop()
                inputNode.removeTap(onBus: 0)
                
                // B. é‡å»ºå¼•æ“ (ç¢ºä¿ä¸‹ä¸€æ¬¡éŒ„éŸ³æ˜¯å…¨æ–°çš„ç‹€æ…‹)
                self.audioEngine.reset()
                self.audioEngine = AVAudioEngine()
                print("âœ… [Audio] å¼•æ“å·²é‡å»º")
                
                self.recognitionRequest = nil
                self.recognitionTask = nil
                
                let finalText = self.recognizedText
                
                DispatchQueue.main.async {
                    self.isRecording = false
                    self.statusMessage = "éŒ„éŸ³çµæŸ"
                    
                    // C. é—œé–‰ Session (æ›æ–·é›»è©±)
                    // é€™ä¸€æ­¥æœƒåˆ‡æ–·æ‰€æœ‰éŸ³è¨Šé€£ç·šï¼Œå°è‡´èˆŠçš„ TTS å¯¦é«”å¤±æ•ˆ
                    let session = AVAudioSession.sharedInstance()
                    do {
                        try session.setActive(false, options: .notifyOthersOnDeactivation)
                        print("âœ… [Audio] Session å·²åœç”¨ (setActive: false)")
                    } catch {
                        print("âš ï¸ [Audio] åœç”¨å¤±æ•—: \(error)")
                    }
                    
                    // D. å»¶é²å¾Œæ’­æ”¾
                    // çµ¦ç³»çµ± 1.0 ç§’çš„æ™‚é–“é‡‹æ”¾éº¥å…‹é¢¨é–å®š
                    print("â³ [Wait] ç­‰å¾… 1.0 ç§’...")
                    DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) {
                        if !finalText.isEmpty {
                            self.speakText()
                        } else {
                            self.statusMessage = "æ²’æœ‰è½åˆ°è²éŸ³"
                        }
                    }
                }
            }
        }
        
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
            self.recognitionRequest?.append(buffer)
        }
        
        audioEngine.prepare()
        do {
            try audioEngine.start()
            isRecording = true
            statusMessage = "æ­£åœ¨éŒ„éŸ³..."
            print("âœ… [Audio] éŒ„éŸ³é–‹å§‹")
        } catch {
            statusMessage = "å¼•æ“å•Ÿå‹•å¤±æ•—"
        }
    }
    
    func stopRecording() {
        print("ğŸ›‘ [User] åœæ­¢éŒ„éŸ³")
        // é€™æœƒè§¸ç™¼ä¸Šé¢çš„ recognitionTask é–‰åŒ…ï¼ŒåŸ·è¡Œæ¸…ç†èˆ‡æ’­æ”¾æµç¨‹
        recognitionRequest?.endAudio()
        statusMessage = "è™•ç†ä¸­..."
    }
    
    // MARK: - æœ—è®€åŠŸèƒ½ (æ ¸å½ˆç´šä¿®å¾©)
    
    func speakText() {
        print("========== é–‹å§‹æœ—è®€æµç¨‹ ==========")
        
        // 1. è¨­å®š Session ç‚º .playback
        let audioSession = AVAudioSession.sharedInstance()
        do {
            try audioSession.setCategory(.playback, mode: .default, options: [])
            try audioSession.setActive(true, options: [])
            print("âœ… [Audio] Session è¨­å®šç‚º .playback")
        } catch {
            print("âŒ [Audio] è¨­å®šå¤±æ•—: \(error)")
            statusMessage = "éŸ³è¨ŠéŒ¯èª¤"
            return
        }
        
        // 2. ğŸ”¥ [å”¯ä¸€è§£æ³•] å¼·åˆ¶å»ºç«‹æ–°çš„ Synthesizer
        // å› ç‚ºèˆŠçš„å¯¦é«”åœ¨ Session æ–·é–‹å¾Œå·²ç¶“å¤±æ•ˆï¼Œå¿…é ˆæ›æ–°çš„æ‰èƒ½æ‹¿åˆ°æ–°çš„ Connection ID
        print("ğŸ”„ [TTS] é‡å»º AVSpeechSynthesizer...")
        speechSynthesizer = AVSpeechSynthesizer()
        speechSynthesizer.delegate = self
        
        // 3. æ’­æ”¾
        let utterance = AVSpeechUtterance(string: recognizedText)
        utterance.voice = AVSpeechSynthesisVoice(language: "zh-TW")
        utterance.rate = 0.5
        
        print("ğŸ”Š [TTS] å‘¼å« speak: \(recognizedText)")
        statusMessage = "æ­£åœ¨æœ—è®€..."
        speechSynthesizer.speak(utterance)
    }
    
    // MARK: - Delegate
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didFinish utterance: AVSpeechUtterance) {
        print("âœ… [TTS] æœ—è®€å®Œæˆ")
        DispatchQueue.main.async {
            self.statusMessage = "æœ—è®€å®Œæˆ"
        }
    }
}

// MARK: - View
struct AudioTestView: View {
    @StateObject private var vm = AudioTestViewModel()
    
    var body: some View {
        VStack(spacing: 30) {
            Text("Audio Crash é©—è­‰å™¨ (æœ€çµ‚ç‰ˆ)")
                .font(.largeTitle)
                .bold()
                .padding(.top)
            
            Text(vm.statusMessage)
                .font(.headline)
                .foregroundColor(.gray)
            
            ScrollView {
                Text(vm.recognizedText.isEmpty ? "..." : vm.recognizedText)
                    .font(.title2)
                    .padding()
                    .frame(maxWidth: .infinity)
            }
            .frame(height: 150)
            .background(Color.gray.opacity(0.1))
            .cornerRadius(10)
            .padding(.horizontal)
            
            Button(action: {
                vm.toggleRecording()
            }) {
                VStack {
                    Image(systemName: vm.isRecording ? "stop.circle.fill" : "mic.circle.fill")
                        .resizable()
                        .frame(width: 80, height: 80)
                        .foregroundColor(vm.isRecording ? .red : .blue)
                    Text(vm.isRecording ? "åœæ­¢éŒ„éŸ³" : "é–‹å§‹éŒ„éŸ³")
                }
            }
            
            Spacer()
            
            Text("ç­–ç•¥ï¼šå¼·åˆ¶é‡å»º Synthesizer\nè§£æ±º -66748 èˆ‡ mDataByteSize 0")
                .font(.caption)
                .foregroundColor(.secondary)
                .multilineTextAlignment(.center)
                .padding(.bottom)
        }
    }
}
